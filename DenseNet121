{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DenseNet121","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1WehZrcbILOn"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["##import"],"metadata":{"id":"ZSzh2LqVQJ-f"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchsummary import summary\n","from torch import optim\n","import numpy as np\n","import pandas as pd\n","\n","# dataset and transformation\n","from torchvision import datasets\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","import os\n","\n","# display images\n","from torchvision import utils\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# utils\n","import numpy as np\n","from torchsummary import summary\n","import time\n","import copy"],"metadata":{"id":"BrPZqOmaI4aV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Load 1"],"metadata":{"id":"ZT_IUyhRQcer"}},{"cell_type":"code","source":["path2data = '/content/drive/Shareddrives/캡스톤 디자인1/junghoon_512'\n","from glob import glob\n","file_data = glob(path2data+'/*')"],"metadata":{"id":"3IwD2SMQQcJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_path =list()\n","for path in file_data:\n","  infile_image_path = glob(path+'/*.png')\n","  image_path+=infile_image_path\n","image_path = sorted(image_path)"],"metadata":{"id":"ReXNYTehQyvc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_data"],"metadata":{"id":"BCwD_kCbQ6ir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(image_path)"],"metadata":{"id":"MxSgCx5OSAve"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data Load 2"],"metadata":{"id":"gNbJhOQ4UP1X"}},{"cell_type":"code","source":["path_csv = '/content/drive/Shareddrives/캡스톤 디자인1/processed_data_entry.csv'\n","df = pd.read_csv(open(path_csv))"],"metadata":{"id":"8-03uVsbUb2n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"id":"McX5MrQwVYIw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=df.iloc[:,:2]\n","df"],"metadata":{"id":"hyuZqJ7mhbdQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_NoFinding_drop= df[df['Finding Labels']!='No Finding']\n","df_NoFinding_drop"],"metadata":{"id":"TgxlzbdJV45F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"LdnXWaUalHsf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data split"],"metadata":{"id":"aZmn5oJrTq26"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","train_path, val_path = train_test_split(image_path,test_size = 0.2,random_state = 42,stratify = df['Finding Labels'])\n","stratify = labeling_df['Finding Labels'])"],"metadata":{"id":"vZi1pJ0GTt7_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"kqaBCoAdSOgD"}},{"cell_type":"markdown","source":["## BottleNeck\n"],"metadata":{"id":"Ym2cbKJKQVAq"}},{"cell_type":"code","source":["class BottleNeck(nn.Module):\n","    def __init__(self, in_channels, growth_rate):\n","        super().__init__()\n","        inner_channels = 4 * growth_rate\n","\n","        self.residual = nn.Sequential(\n","            nn.BatchNorm2d(in_channels),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels, inner_channels, 1, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(inner_channels),\n","            nn.ReLU(),\n","            nn.Conv2d(inner_channels, growth_rate, 3, stride=1, padding=1, bias=False)\n","        )\n","\n","        self.shortcut = nn.Sequential()\n","\n","    def forward(self, x):\n","        return torch.cat([self.shortcut(x), self.residual(x)], 1)\n","        ##확인"],"metadata":{"id":"3Hy8lyfEL9RW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transition Block: feature map과 채널의 수를 줄인다.\n","class Transition(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","\n","        self.down_sample = nn.Sequential(\n","            nn.BatchNorm2d(in_channels),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False),\n","            nn.AvgPool2d(2, stride=2)\n","        )\n","\n","    def forward(self, x):\n","        return self.down_sample(x)"],"metadata":{"id":"68A88xuBL9_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DenseNet\n","class DenseNet(nn.Module):\n","    def __init__(self, nblocks, growth_rate=12, reduction=0.5, num_classes=11, init_weights=True):\n","        super().__init__()\n","\n","        self.growth_rate = growth_rate\n","        inner_channels = 2 * growth_rate # output channels of conv1 before entering Dense Block\n","\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(1, inner_channels, 7, stride=2, padding=3),\n","            nn.MaxPool2d(3, stride=2, padding=1)\n","        )\n","\n","        self.features = nn.Sequential()\n","\n","        for i in range(len(nblocks)-1):\n","            self.features.add_module('dense_block_{}'.format(i), self._make_dense_block(nblocks[i], inner_channels))\n","            inner_channels += growth_rate * nblocks[i]\n","            out_channels = int(reduction * inner_channels)\n","            self.features.add_module('transition_layer_{}'.format(i), Transition(inner_channels, out_channels))\n","            inner_channels = out_channels \n","        \n","        self.features.add_module('dense_block_{}'.format(len(nblocks)-1), self._make_dense_block(nblocks[len(nblocks)-1], inner_channels))\n","        inner_channels += growth_rate * nblocks[len(nblocks)-1]\n","        self.features.add_module('bn', nn.BatchNorm2d(inner_channels))\n","        self.features.add_module('relu', nn.ReLU())\n","\n","        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n","        self.linear = nn.Linear(inner_channels, num_classes)\n","\n","        # weight initialization\n","        if init_weights:\n","            self._initialize_weights()\n","    \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.features(x)\n","        x = self.avg_pool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.linear(x)\n","        return x\n","\n","    def _make_dense_block(self, nblock, inner_channels):\n","        dense_block = nn.Sequential()\n","        for i in range(nblock):\n","            dense_block.add_module('bottle_neck_layer_{}'.format(i), BottleNeck(inner_channels, self.growth_rate))\n","            inner_channels += self.growth_rate\n","        return dense_block\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n","\n","def DenseNet_121():\n","    return DenseNet([6, 12, 24, 6]) ## Dense block의 갯수를 입력"],"metadata":{"id":"6Uvea10rL_iG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check model\n","x = torch.randn(1, 1, 256, 256)\n","model = DenseNet_121()\n","output = model(x)\n","print(output.size())"],"metadata":{"id":"EHthPJN42etN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print model summary\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","summary(model, (1, 512, 512), device=device.type) "],"metadata":{"id":"wozo0KzJ2g0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device"],"metadata":{"id":"DOGjS1ZT4Abg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training "],"metadata":{"id":"X26DTuQceGOv"}},{"cell_type":"code","source":["loss_func = nn.CrossEntropyLoss(reduction='sum')\n","opt = optim.Adam(model.parameters(), lr=0.01)\n","\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=8)\n","\n","\n","# get current lr\n","def get_lr(opt):\n","    for param_group in opt.param_groups:\n","        return param_group['lr']\n","\n","\n","# calculate the metric per mini-batch\n","def metric_batch(output, target):\n","    pred = output.argmax(1, keepdim=True)\n","    corrects = pred.eq(target.view_as(pred)).sum().item()\n","    return corrects\n","\n","\n","# calculate the loss per mini-batch\n","def loss_batch(loss_func, output, target, opt=None):\n","    loss_b = loss_func(output, target)\n","    metric_b = metric_batch(output, target)\n","\n","    if opt is not None:\n","        opt.zero_grad()\n","        loss_b.backward()\n","        opt.step()\n","    \n","    return loss_b.item(), metric_b\n","\n","\n","# calculate the loss per epochs\n","def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n","    running_loss = 0.0\n","    running_metric = 0.0\n","    len_data = len(dataset_dl.dataset)\n","\n","    for xb, yb in dataset_dl:\n","        xb = xb.to(device)\n","        yb = yb.to(device)\n","        output = model(xb)\n","\n","        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n","\n","        running_loss += loss_b\n","        \n","        if metric_b is not None:\n","            running_metric += metric_b\n","\n","        if sanity_check is True:\n","            break\n","\n","    loss = running_loss / len_data\n","    metric = running_metric / len_data\n","    return loss, metric\n","\n","\n","# function to start training\n","def train_val(model, params):\n","    num_epochs=params['num_epochs']\n","    loss_func=params['loss_func']\n","    opt=params['optimizer']\n","    train_dl=params['train_dl']\n","    val_dl=params['val_dl']\n","    sanity_check=params['sanity_check']\n","    lr_scheduler=params['lr_scheduler']\n","    path2weights=params['path2weights']\n","\n","    loss_history = {'train': [], 'val': []}\n","    metric_history = {'train': [], 'val': []}\n","\n","    best_loss = float('inf')\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    start_time = time.time()\n","\n","    for epoch in range(num_epochs):\n","        current_lr = get_lr(opt)\n","        print('Epoch {}/{}, current lr= {}'.format(epoch, num_epochs-1, current_lr))\n","\n","        model.train()\n","        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n","        loss_history['train'].append(train_loss)\n","        metric_history['train'].append(train_metric)\n","\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n","        loss_history['val'].append(val_loss)\n","        metric_history['val'].append(val_metric)\n","\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            best_model_wts = copy.deepcopy(model.state_dict())\n","            torch.save(model.state_dict(), path2weights)\n","            print('Copied best model weights!')\n","\n","        lr_scheduler.step(val_loss)\n","        if current_lr != get_lr(opt):\n","            print('Loading best model weights!')\n","            model.load_state_dict(best_model_wts)\n","\n","        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n","        print('-'*10)\n","\n","    model.load_state_dict(best_model_wts)\n","    return model, loss_history, metric_history"],"metadata":{"id":"JJ4OlXluPipF"},"execution_count":null,"outputs":[]}]}